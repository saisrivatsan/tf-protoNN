{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cfgs.config_eurlex as config\n",
    "from trainer.single_gpu_train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.cfg.use_pre_init = 'datasets/eurlex/init_params_faiss.mat'\n",
    "m = Trainer(config.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /n/home13/sravindranath/.conda/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-28 23:40:57,192:WARNING:From /n/home13/sravindranath/.conda/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "2018-12-28 23:41:15,173:INFO:TRAIN-BATCH Iter = 10, t = 5.30, Loss = 455.07, Prec@1: 0.82, Prec@3: 0.65, Prec@5: 0.53\n",
      "2018-12-28 23:41:16,998:INFO:TRAIN-BATCH Iter = 20, t = 7.07, Loss = 442.62, Prec@1: 0.85, Prec@3: 0.71, Prec@5: 0.59\n",
      "2018-12-28 23:41:18,827:INFO:TRAIN-BATCH Iter = 30, t = 8.84, Loss = 421.58, Prec@1: 0.86, Prec@3: 0.73, Prec@5: 0.61\n",
      "2018-12-28 23:41:20,655:INFO:TRAIN-BATCH Iter = 40, t = 10.60, Loss = 401.93, Prec@1: 0.89, Prec@3: 0.74, Prec@5: 0.61\n",
      "2018-12-28 23:41:22,485:INFO:TRAIN-BATCH Iter = 50, t = 12.37, Loss = 395.03, Prec@1: 0.89, Prec@3: 0.76, Prec@5: 0.66\n",
      "2018-12-28 23:41:24,319:INFO:TRAIN-BATCH Iter = 60, t = 14.15, Loss = 388.98, Prec@1: 0.89, Prec@3: 0.75, Prec@5: 0.63\n",
      "2018-12-28 23:41:26,153:INFO:TRAIN-BATCH Iter = 70, t = 15.92, Loss = 382.22, Prec@1: 0.91, Prec@3: 0.79, Prec@5: 0.67\n",
      "2018-12-28 23:41:27,990:INFO:TRAIN-BATCH Iter = 80, t = 17.70, Loss = 371.78, Prec@1: 0.91, Prec@3: 0.77, Prec@5: 0.67\n",
      "2018-12-28 23:41:29,823:INFO:TRAIN-BATCH Iter = 90, t = 19.47, Loss = 381.47, Prec@1: 0.93, Prec@3: 0.79, Prec@5: 0.70\n",
      "2018-12-28 23:41:35,077:INFO:TRAIN-BATCH Iter = 100, t = 21.24, Loss = 382.72, Prec@1: 0.92, Prec@3: 0.78, Prec@5: 0.67\n",
      "2018-12-28 23:41:35,228:INFO:VAL-ALL Iter = 100, t = 0.15, Loss = 396.35, Prec@1: 0.88, Prec@3: 0.74, Prec@5: 0.63\n",
      "2018-12-28 23:41:37,076:INFO:TRAIN-BATCH Iter = 110, t = 23.03, Loss = 364.83, Prec@1: 0.93, Prec@3: 0.80, Prec@5: 0.68\n",
      "2018-12-28 23:41:38,907:INFO:TRAIN-BATCH Iter = 120, t = 24.81, Loss = 383.08, Prec@1: 0.92, Prec@3: 0.77, Prec@5: 0.66\n",
      "2018-12-28 23:41:40,742:INFO:TRAIN-BATCH Iter = 130, t = 26.58, Loss = 354.11, Prec@1: 0.93, Prec@3: 0.80, Prec@5: 0.70\n",
      "2018-12-28 23:41:42,574:INFO:TRAIN-BATCH Iter = 140, t = 28.35, Loss = 357.62, Prec@1: 0.95, Prec@3: 0.84, Prec@5: 0.73\n",
      "2018-12-28 23:41:44,406:INFO:TRAIN-BATCH Iter = 150, t = 30.12, Loss = 358.54, Prec@1: 0.91, Prec@3: 0.81, Prec@5: 0.69\n",
      "2018-12-28 23:41:46,238:INFO:TRAIN-BATCH Iter = 160, t = 31.89, Loss = 368.07, Prec@1: 0.91, Prec@3: 0.80, Prec@5: 0.71\n",
      "2018-12-28 23:41:48,071:INFO:TRAIN-BATCH Iter = 170, t = 33.67, Loss = 336.52, Prec@1: 0.93, Prec@3: 0.80, Prec@5: 0.69\n",
      "2018-12-28 23:41:49,903:INFO:TRAIN-BATCH Iter = 180, t = 35.44, Loss = 367.40, Prec@1: 0.95, Prec@3: 0.83, Prec@5: 0.72\n",
      "2018-12-28 23:41:51,734:INFO:TRAIN-BATCH Iter = 190, t = 37.21, Loss = 363.96, Prec@1: 0.93, Prec@3: 0.82, Prec@5: 0.71\n",
      "2018-12-28 23:41:56,818:INFO:TRAIN-BATCH Iter = 200, t = 38.99, Loss = 346.98, Prec@1: 0.95, Prec@3: 0.82, Prec@5: 0.69\n",
      "2018-12-28 23:41:56,964:INFO:VAL-ALL Iter = 200, t = 0.14, Loss = 396.70, Prec@1: 0.88, Prec@3: 0.75, Prec@5: 0.63\n",
      "2018-12-28 23:41:58,788:INFO:TRAIN-BATCH Iter = 210, t = 40.75, Loss = 353.37, Prec@1: 0.95, Prec@3: 0.83, Prec@5: 0.71\n",
      "2018-12-28 23:42:00,628:INFO:TRAIN-BATCH Iter = 220, t = 42.53, Loss = 351.23, Prec@1: 0.94, Prec@3: 0.84, Prec@5: 0.72\n",
      "2018-12-28 23:42:02,458:INFO:TRAIN-BATCH Iter = 230, t = 44.30, Loss = 348.40, Prec@1: 0.96, Prec@3: 0.86, Prec@5: 0.73\n",
      "2018-12-28 23:42:04,288:INFO:TRAIN-BATCH Iter = 240, t = 46.07, Loss = 351.33, Prec@1: 0.96, Prec@3: 0.86, Prec@5: 0.73\n",
      "2018-12-28 23:42:06,109:INFO:TRAIN-BATCH Iter = 250, t = 47.84, Loss = 349.62, Prec@1: 0.96, Prec@3: 0.86, Prec@5: 0.72\n",
      "2018-12-28 23:42:07,942:INFO:TRAIN-BATCH Iter = 260, t = 49.62, Loss = 352.59, Prec@1: 0.96, Prec@3: 0.85, Prec@5: 0.74\n",
      "2018-12-28 23:42:09,781:INFO:TRAIN-BATCH Iter = 270, t = 51.39, Loss = 353.96, Prec@1: 0.95, Prec@3: 0.83, Prec@5: 0.73\n",
      "2018-12-28 23:42:11,604:INFO:TRAIN-BATCH Iter = 280, t = 53.16, Loss = 346.09, Prec@1: 0.97, Prec@3: 0.89, Prec@5: 0.77\n",
      "2018-12-28 23:42:13,448:INFO:TRAIN-BATCH Iter = 290, t = 54.94, Loss = 341.57, Prec@1: 0.96, Prec@3: 0.88, Prec@5: 0.76\n",
      "2018-12-28 23:42:18,594:INFO:TRAIN-BATCH Iter = 300, t = 56.72, Loss = 341.37, Prec@1: 0.95, Prec@3: 0.87, Prec@5: 0.76\n",
      "2018-12-28 23:42:18,741:INFO:VAL-ALL Iter = 300, t = 0.14, Loss = 399.97, Prec@1: 0.88, Prec@3: 0.74, Prec@5: 0.62\n",
      "2018-12-28 23:42:20,571:INFO:TRAIN-BATCH Iter = 310, t = 58.49, Loss = 338.51, Prec@1: 0.96, Prec@3: 0.85, Prec@5: 0.74\n",
      "2018-12-28 23:42:22,409:INFO:TRAIN-BATCH Iter = 320, t = 60.26, Loss = 339.76, Prec@1: 0.95, Prec@3: 0.86, Prec@5: 0.75\n",
      "2018-12-28 23:42:24,243:INFO:TRAIN-BATCH Iter = 330, t = 62.04, Loss = 343.28, Prec@1: 0.96, Prec@3: 0.85, Prec@5: 0.73\n",
      "2018-12-28 23:42:26,080:INFO:TRAIN-BATCH Iter = 340, t = 63.81, Loss = 339.07, Prec@1: 0.94, Prec@3: 0.86, Prec@5: 0.76\n",
      "2018-12-28 23:42:27,911:INFO:TRAIN-BATCH Iter = 350, t = 65.58, Loss = 335.64, Prec@1: 0.98, Prec@3: 0.89, Prec@5: 0.77\n",
      "2018-12-28 23:42:29,746:INFO:TRAIN-BATCH Iter = 360, t = 67.36, Loss = 338.66, Prec@1: 0.96, Prec@3: 0.87, Prec@5: 0.74\n",
      "2018-12-28 23:42:31,581:INFO:TRAIN-BATCH Iter = 370, t = 69.13, Loss = 346.73, Prec@1: 0.98, Prec@3: 0.88, Prec@5: 0.76\n",
      "2018-12-28 23:42:33,417:INFO:TRAIN-BATCH Iter = 380, t = 70.90, Loss = 344.87, Prec@1: 0.97, Prec@3: 0.90, Prec@5: 0.79\n",
      "2018-12-28 23:42:35,253:INFO:TRAIN-BATCH Iter = 390, t = 72.68, Loss = 336.04, Prec@1: 0.97, Prec@3: 0.88, Prec@5: 0.77\n",
      "2018-12-28 23:42:40,354:INFO:TRAIN-BATCH Iter = 400, t = 74.45, Loss = 340.89, Prec@1: 0.96, Prec@3: 0.88, Prec@5: 0.76\n",
      "2018-12-28 23:42:40,505:INFO:VAL-ALL Iter = 400, t = 0.15, Loss = 403.61, Prec@1: 0.87, Prec@3: 0.74, Prec@5: 0.62\n",
      "2018-12-28 23:42:42,337:INFO:TRAIN-BATCH Iter = 410, t = 76.23, Loss = 338.62, Prec@1: 0.96, Prec@3: 0.86, Prec@5: 0.75\n",
      "2018-12-28 23:42:44,166:INFO:TRAIN-BATCH Iter = 420, t = 78.00, Loss = 345.25, Prec@1: 0.96, Prec@3: 0.87, Prec@5: 0.75\n",
      "2018-12-28 23:42:46,002:INFO:TRAIN-BATCH Iter = 430, t = 79.77, Loss = 318.57, Prec@1: 0.96, Prec@3: 0.88, Prec@5: 0.77\n",
      "2018-12-28 23:42:47,839:INFO:TRAIN-BATCH Iter = 440, t = 81.55, Loss = 327.61, Prec@1: 0.96, Prec@3: 0.88, Prec@5: 0.76\n",
      "2018-12-28 23:42:49,668:INFO:TRAIN-BATCH Iter = 450, t = 83.32, Loss = 319.83, Prec@1: 0.98, Prec@3: 0.88, Prec@5: 0.76\n",
      "2018-12-28 23:42:51,501:INFO:TRAIN-BATCH Iter = 460, t = 85.09, Loss = 336.98, Prec@1: 0.98, Prec@3: 0.87, Prec@5: 0.75\n",
      "2018-12-28 23:42:53,329:INFO:TRAIN-BATCH Iter = 470, t = 86.86, Loss = 330.00, Prec@1: 0.98, Prec@3: 0.91, Prec@5: 0.79\n",
      "2018-12-28 23:42:55,157:INFO:TRAIN-BATCH Iter = 480, t = 88.63, Loss = 326.20, Prec@1: 0.98, Prec@3: 0.91, Prec@5: 0.76\n",
      "2018-12-28 23:42:56,987:INFO:TRAIN-BATCH Iter = 490, t = 90.40, Loss = 326.53, Prec@1: 0.96, Prec@3: 0.88, Prec@5: 0.76\n",
      "2018-12-28 23:43:02,104:INFO:TRAIN-BATCH Iter = 500, t = 92.17, Loss = 336.11, Prec@1: 0.95, Prec@3: 0.89, Prec@5: 0.78\n",
      "2018-12-28 23:43:02,249:INFO:VAL-ALL Iter = 500, t = 0.14, Loss = 407.72, Prec@1: 0.87, Prec@3: 0.73, Prec@5: 0.61\n",
      "2018-12-28 23:43:04,073:INFO:TRAIN-BATCH Iter = 510, t = 93.94, Loss = 324.57, Prec@1: 0.98, Prec@3: 0.91, Prec@5: 0.79\n",
      "2018-12-28 23:43:05,905:INFO:TRAIN-BATCH Iter = 520, t = 95.71, Loss = 327.05, Prec@1: 0.99, Prec@3: 0.91, Prec@5: 0.80\n",
      "2018-12-28 23:43:07,738:INFO:TRAIN-BATCH Iter = 530, t = 97.48, Loss = 323.78, Prec@1: 0.98, Prec@3: 0.91, Prec@5: 0.79\n",
      "2018-12-28 23:43:09,571:INFO:TRAIN-BATCH Iter = 540, t = 99.26, Loss = 330.28, Prec@1: 0.99, Prec@3: 0.91, Prec@5: 0.80\n",
      "2018-12-28 23:43:11,404:INFO:TRAIN-BATCH Iter = 550, t = 101.03, Loss = 324.84, Prec@1: 0.99, Prec@3: 0.92, Prec@5: 0.80\n",
      "2018-12-28 23:43:13,238:INFO:TRAIN-BATCH Iter = 560, t = 102.81, Loss = 334.92, Prec@1: 0.98, Prec@3: 0.90, Prec@5: 0.81\n",
      "2018-12-28 23:43:15,072:INFO:TRAIN-BATCH Iter = 570, t = 104.58, Loss = 338.40, Prec@1: 0.96, Prec@3: 0.88, Prec@5: 0.78\n",
      "2018-12-28 23:43:16,905:INFO:TRAIN-BATCH Iter = 580, t = 106.35, Loss = 325.56, Prec@1: 0.98, Prec@3: 0.91, Prec@5: 0.80\n",
      "2018-12-28 23:43:18,739:INFO:TRAIN-BATCH Iter = 590, t = 108.12, Loss = 327.10, Prec@1: 0.99, Prec@3: 0.92, Prec@5: 0.81\n",
      "2018-12-28 23:43:23,838:INFO:TRAIN-BATCH Iter = 600, t = 109.90, Loss = 315.38, Prec@1: 0.98, Prec@3: 0.92, Prec@5: 0.82\n",
      "2018-12-28 23:43:23,980:INFO:VAL-ALL Iter = 600, t = 0.14, Loss = 411.42, Prec@1: 0.86, Prec@3: 0.73, Prec@5: 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-28 23:43:25,807:INFO:TRAIN-BATCH Iter = 610, t = 111.66, Loss = 327.04, Prec@1: 0.97, Prec@3: 0.89, Prec@5: 0.78\n",
      "2018-12-28 23:43:27,638:INFO:TRAIN-BATCH Iter = 620, t = 113.44, Loss = 315.46, Prec@1: 0.98, Prec@3: 0.91, Prec@5: 0.80\n",
      "2018-12-28 23:43:29,474:INFO:TRAIN-BATCH Iter = 630, t = 115.21, Loss = 328.47, Prec@1: 0.97, Prec@3: 0.88, Prec@5: 0.78\n",
      "2018-12-28 23:43:31,312:INFO:TRAIN-BATCH Iter = 640, t = 116.99, Loss = 328.95, Prec@1: 0.96, Prec@3: 0.90, Prec@5: 0.80\n",
      "2018-12-28 23:43:33,142:INFO:TRAIN-BATCH Iter = 650, t = 118.76, Loss = 309.85, Prec@1: 0.98, Prec@3: 0.89, Prec@5: 0.78\n",
      "2018-12-28 23:43:34,972:INFO:TRAIN-BATCH Iter = 660, t = 120.53, Loss = 323.69, Prec@1: 0.99, Prec@3: 0.91, Prec@5: 0.79\n",
      "2018-12-28 23:43:36,804:INFO:TRAIN-BATCH Iter = 670, t = 122.30, Loss = 331.82, Prec@1: 0.98, Prec@3: 0.90, Prec@5: 0.79\n",
      "2018-12-28 23:43:38,635:INFO:TRAIN-BATCH Iter = 680, t = 124.08, Loss = 316.07, Prec@1: 0.98, Prec@3: 0.92, Prec@5: 0.80\n",
      "2018-12-28 23:43:40,460:INFO:TRAIN-BATCH Iter = 690, t = 125.84, Loss = 307.50, Prec@1: 0.99, Prec@3: 0.91, Prec@5: 0.80\n",
      "2018-12-28 23:43:45,572:INFO:TRAIN-BATCH Iter = 700, t = 127.62, Loss = 314.51, Prec@1: 0.98, Prec@3: 0.91, Prec@5: 0.79\n",
      "2018-12-28 23:43:45,725:INFO:VAL-ALL Iter = 700, t = 0.15, Loss = 414.75, Prec@1: 0.86, Prec@3: 0.73, Prec@5: 0.60\n",
      "2018-12-28 23:43:47,558:INFO:TRAIN-BATCH Iter = 710, t = 129.39, Loss = 322.75, Prec@1: 0.98, Prec@3: 0.92, Prec@5: 0.80\n",
      "2018-12-28 23:43:49,397:INFO:TRAIN-BATCH Iter = 720, t = 131.17, Loss = 307.22, Prec@1: 0.97, Prec@3: 0.90, Prec@5: 0.78\n",
      "2018-12-28 23:43:51,233:INFO:TRAIN-BATCH Iter = 730, t = 132.95, Loss = 319.74, Prec@1: 0.98, Prec@3: 0.90, Prec@5: 0.79\n",
      "2018-12-28 23:43:53,067:INFO:TRAIN-BATCH Iter = 740, t = 134.72, Loss = 334.88, Prec@1: 0.99, Prec@3: 0.92, Prec@5: 0.81\n",
      "2018-12-28 23:43:54,900:INFO:TRAIN-BATCH Iter = 750, t = 136.49, Loss = 321.83, Prec@1: 0.99, Prec@3: 0.91, Prec@5: 0.79\n",
      "2018-12-28 23:43:56,734:INFO:TRAIN-BATCH Iter = 760, t = 138.27, Loss = 323.68, Prec@1: 0.98, Prec@3: 0.93, Prec@5: 0.82\n",
      "2018-12-28 23:43:58,565:INFO:TRAIN-BATCH Iter = 770, t = 140.04, Loss = 315.34, Prec@1: 0.99, Prec@3: 0.92, Prec@5: 0.81\n",
      "2018-12-28 23:44:00,403:INFO:TRAIN-BATCH Iter = 780, t = 141.82, Loss = 313.89, Prec@1: 0.98, Prec@3: 0.93, Prec@5: 0.82\n",
      "2018-12-28 23:44:02,238:INFO:TRAIN-BATCH Iter = 790, t = 143.59, Loss = 311.59, Prec@1: 0.99, Prec@3: 0.92, Prec@5: 0.81\n",
      "2018-12-28 23:44:07,355:INFO:TRAIN-BATCH Iter = 800, t = 145.37, Loss = 306.01, Prec@1: 0.98, Prec@3: 0.94, Prec@5: 0.83\n",
      "2018-12-28 23:44:07,503:INFO:VAL-ALL Iter = 800, t = 0.14, Loss = 417.24, Prec@1: 0.86, Prec@3: 0.72, Prec@5: 0.60\n",
      "2018-12-28 23:44:09,329:INFO:TRAIN-BATCH Iter = 810, t = 147.14, Loss = 304.01, Prec@1: 0.98, Prec@3: 0.90, Prec@5: 0.79\n",
      "2018-12-28 23:44:11,162:INFO:TRAIN-BATCH Iter = 820, t = 148.91, Loss = 326.18, Prec@1: 0.98, Prec@3: 0.90, Prec@5: 0.81\n",
      "2018-12-28 23:44:12,998:INFO:TRAIN-BATCH Iter = 830, t = 150.68, Loss = 307.67, Prec@1: 0.99, Prec@3: 0.91, Prec@5: 0.82\n",
      "2018-12-28 23:44:14,833:INFO:TRAIN-BATCH Iter = 840, t = 152.46, Loss = 304.92, Prec@1: 0.97, Prec@3: 0.92, Prec@5: 0.81\n",
      "2018-12-28 23:44:16,666:INFO:TRAIN-BATCH Iter = 850, t = 154.23, Loss = 316.07, Prec@1: 0.98, Prec@3: 0.93, Prec@5: 0.84\n",
      "2018-12-28 23:44:18,500:INFO:TRAIN-BATCH Iter = 860, t = 156.00, Loss = 315.11, Prec@1: 1.00, Prec@3: 0.92, Prec@5: 0.81\n",
      "2018-12-28 23:44:20,330:INFO:TRAIN-BATCH Iter = 870, t = 157.77, Loss = 309.58, Prec@1: 0.99, Prec@3: 0.93, Prec@5: 0.82\n",
      "2018-12-28 23:44:22,162:INFO:TRAIN-BATCH Iter = 880, t = 159.55, Loss = 316.08, Prec@1: 0.98, Prec@3: 0.94, Prec@5: 0.82\n",
      "2018-12-28 23:44:23,989:INFO:TRAIN-BATCH Iter = 890, t = 161.31, Loss = 322.27, Prec@1: 0.97, Prec@3: 0.93, Prec@5: 0.82\n",
      "2018-12-28 23:44:29,094:INFO:TRAIN-BATCH Iter = 900, t = 163.09, Loss = 302.33, Prec@1: 0.97, Prec@3: 0.92, Prec@5: 0.80\n",
      "2018-12-28 23:44:29,243:INFO:VAL-ALL Iter = 900, t = 0.14, Loss = 420.57, Prec@1: 0.85, Prec@3: 0.72, Prec@5: 0.60\n",
      "2018-12-28 23:44:31,067:INFO:TRAIN-BATCH Iter = 910, t = 164.85, Loss = 305.77, Prec@1: 0.98, Prec@3: 0.93, Prec@5: 0.83\n",
      "2018-12-28 23:44:32,906:INFO:TRAIN-BATCH Iter = 920, t = 166.63, Loss = 317.96, Prec@1: 0.99, Prec@3: 0.93, Prec@5: 0.83\n",
      "2018-12-28 23:44:34,739:INFO:TRAIN-BATCH Iter = 930, t = 168.40, Loss = 295.89, Prec@1: 0.99, Prec@3: 0.94, Prec@5: 0.82\n",
      "2018-12-28 23:44:36,571:INFO:TRAIN-BATCH Iter = 940, t = 170.18, Loss = 318.73, Prec@1: 1.00, Prec@3: 0.93, Prec@5: 0.81\n",
      "2018-12-28 23:44:38,400:INFO:TRAIN-BATCH Iter = 950, t = 171.94, Loss = 320.99, Prec@1: 0.99, Prec@3: 0.92, Prec@5: 0.83\n",
      "2018-12-28 23:44:40,233:INFO:TRAIN-BATCH Iter = 960, t = 173.72, Loss = 308.58, Prec@1: 1.00, Prec@3: 0.95, Prec@5: 0.83\n",
      "2018-12-28 23:44:42,066:INFO:TRAIN-BATCH Iter = 970, t = 175.49, Loss = 311.38, Prec@1: 0.98, Prec@3: 0.92, Prec@5: 0.81\n",
      "2018-12-28 23:44:43,902:INFO:TRAIN-BATCH Iter = 980, t = 177.27, Loss = 301.51, Prec@1: 0.99, Prec@3: 0.93, Prec@5: 0.80\n",
      "2018-12-28 23:44:45,736:INFO:TRAIN-BATCH Iter = 990, t = 179.04, Loss = 306.87, Prec@1: 0.98, Prec@3: 0.93, Prec@5: 0.83\n",
      "2018-12-28 23:44:50,846:INFO:TRAIN-BATCH Iter = 1000, t = 180.81, Loss = 314.37, Prec@1: 1.00, Prec@3: 0.94, Prec@5: 0.82\n",
      "2018-12-28 23:44:50,997:INFO:VAL-ALL Iter = 1000, t = 0.15, Loss = 423.50, Prec@1: 0.85, Prec@3: 0.72, Prec@5: 0.60\n",
      "2018-12-28 23:44:51,115:INFO:TEST-ALL Iter = 1000, t = 0.11, Loss = 465.92, Prec@1: 0.79, Prec@3: 0.65, Prec@5: 0.53\n"
     ]
    }
   ],
   "source": [
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
